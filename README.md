

# Survey
- The Deep Learning Compiler: A Comprehensive Survey


# Deep Learning Framework
- [TensorFlow-XLA]
- [Pytorch Glow]Glow: Graph lowering compiler techniques for neural networks
- TVM: An automated end-to-end optimizing compiler for deep learning 


# Deep Learning Hardware


# Hareware-specific Deep Learning Code Generator
- FPGA:
- ASIC

# Front-End(High-Level IR)


# Back-End(Low-Level IR)
- HeteroCL: A Multi-Paradigm Programming Infrastructure for Software-Defined Reconfigurable Computing
- HeteroFlow: An Accelerator Programming Model with Decoupled Data Placement for Software-Defined FPGAs
- Halide: a language and compiler for optimizing parallelism, locality, and recomputation in image processing pipelines
- DLVM: A modern compiler infrastructure for deep learning systems
- FFTW: An adaptive software architecture for the FFT
- Automatically Scheduling Halide Image Processing Pipelines

# Auto-tuning Model Compilation
- AutoTVM Learning to optimize tensor programs
- Opentuner: An extensible framework for program autotuning
- Efficient Hyperparameter Optimization and Infinitely Many Armed Bandits


# DSP for Machine Learning 
- OptiML: An Implicitly Parallel Domain-Specific Language for Machine Learning


# Code Generation
- LLVM
- CUDA
- OpenCL
- OpenGL
- Polyhedral Parallel Code Generation for CUDA
- Lift: A Functional Data-Parallel IR for High-Performance GPU Code Generation


# Others
- Futhark: Purely Functional GPU-Programming with Nested Parallelism and In-Place Array Updates


# References
- https://github.com/zwang4/awesome-machine-learning-in-compilers
- https://github.com/merrymercy/awesome-tensor-compilers
